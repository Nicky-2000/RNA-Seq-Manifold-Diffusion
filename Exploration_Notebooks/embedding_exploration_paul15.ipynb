{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fca7d56",
   "metadata": {},
   "source": [
    "# Preprocess paul15 scRNA-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b30f8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Core scientific stack\n",
    "# %pip install --quiet numpy scipy pandas matplotlib scikit-learn\n",
    "\n",
    "# # Scanpy for single-cell analysis\n",
    "# %pip install --quiet scanpy\n",
    "\n",
    "# # PHATE for manifold learning\n",
    "# %pip install --quiet phate\n",
    "\n",
    "# # scVI for latent representations\n",
    "# %pip install --quiet scvi-tools\n",
    "\n",
    "# # (Optional, but often useful)\n",
    "# %pip install --quiet umap-learn  # if you ever call UMAP directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad3da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75cc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_energy.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import sparse\n",
    "import scanpy as sc  # only for type hints; not strictly necessary\n",
    "\n",
    "\n",
    "class AnnDataExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps an AnnData object's X matrix (after prep()) as a PyTorch dataset.\n",
    "    Uses HVG, log-normalized expression directly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ad: \"sc.AnnData\", float_dtype: np.dtype = np.float32):\n",
    "        X = ad.X\n",
    "        if sparse.issparse(X):\n",
    "            X = X.toarray()\n",
    "        X = np.asarray(X, dtype=float_dtype)\n",
    "        # Optional: mean-center features for numerical stability\n",
    "        self.mean_ = X.mean(axis=0, keepdims=True)\n",
    "        self.X = X - self.mean_\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return torch.from_numpy(self.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b31a2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffmap_eggfm.py\n",
    "\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse as sp_sparse\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def hessian_quadratic_form_batched(\n",
    "    energy_model,\n",
    "    X_batch: np.ndarray,  # (B, D) points x_b\n",
    "    V_batch: np.ndarray,  # (B, D) directions v_b\n",
    "    device: str,\n",
    "    mode: str = \"Hv_norm2\",\n",
    ") -> np.ndarray:\n",
    "    ...\n",
    "    # unchanged from your version\n",
    "    ...\n",
    "\n",
    "\n",
    "def build_eggfm_diffmap(\n",
    "    ad_prep,\n",
    "    energy_model,\n",
    "    diff_cfg: Dict[str, Any],\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an EGGFM-aware Diffusion Map embedding using a metric induced by\n",
    "    the Hessian of the energy model via batched Hessian-vector products.\n",
    "    \"\"\"\n",
    "    X = ad_prep.X\n",
    "    if sp_sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    n_cells, D = X.shape\n",
    "\n",
    "    n_neighbors = diff_cfg.get(\"n_neighbors\", 30)\n",
    "    n_comps = diff_cfg.get(\"n_comps\", 30)\n",
    "    device = diff_cfg.get(\"device\", \"cuda\")\n",
    "    device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    eps_mode = diff_cfg.get(\"eps_mode\", \"median\")\n",
    "    eps_value = diff_cfg.get(\"eps_value\", 1.0)\n",
    "    hvp_mode = diff_cfg.get(\"hvp_mode\", \"Hv_norm2\")\n",
    "    hvp_batch_size = diff_cfg.get(\"hvp_batch_size\", 1024)\n",
    "    t = diff_cfg.get(\"t\", 1.0)\n",
    "\n",
    "    # Move model to device once\n",
    "    energy_model = energy_model.to(device)\n",
    "    energy_model.eval()\n",
    "\n",
    "    print(f\"[EGGFM DiffMap] X shape: {X.shape}\", flush=True)\n",
    "\n",
    "    # 1) kNN for neighbor selection (Euclidean, only for neighbor indices)\n",
    "    print(\n",
    "        \"[EGGFM DiffMap] building kNN graph (euclidean for neighbor selection)...\",\n",
    "        flush=True,\n",
    "    )\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors + 1, metric=\"euclidean\")\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    # neighbors per cell (excluding self)\n",
    "    k = indices.shape[1] - 1\n",
    "    assert k == n_neighbors, \"indices second dimension should be n_neighbors+1\"\n",
    "\n",
    "    # 2) Flatten edges\n",
    "    rows = np.repeat(np.arange(n_cells, dtype=np.int64), k)\n",
    "    cols = indices[:, 1:].reshape(-1).astype(np.int64)\n",
    "    n_edges = rows.shape[0]\n",
    "\n",
    "    print(f\"[EGGFM DiffMap] total edges (directed): {n_edges}\", flush=True)\n",
    "\n",
    "    # 3) Compute metric-aware edge lengths ℓ_ij^2 via batched HVPs\n",
    "    l2_vals = np.empty(n_edges, dtype=np.float64)\n",
    "\n",
    "    print(\n",
    "        \"[EGGFM DiffMap] computing Hessian-based edge lengths in batches...\", flush=True\n",
    "    )\n",
    "    n_batches = (n_edges + hvp_batch_size - 1) // hvp_batch_size\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        start = b * hvp_batch_size\n",
    "        end = min((b + 1) * hvp_batch_size, n_edges)\n",
    "        if start >= end:\n",
    "            break\n",
    "\n",
    "        i_batch = rows[start:end]\n",
    "        j_batch = cols[start:end]\n",
    "\n",
    "        Xi_batch = X[i_batch]  # (B, D)\n",
    "        Xj_batch = X[j_batch]  # (B, D)\n",
    "        V_batch = Xj_batch - Xi_batch  # (B, D)\n",
    "\n",
    "        q_batch = hessian_quadratic_form_batched(\n",
    "            energy_model,\n",
    "            Xi_batch,\n",
    "            V_batch,\n",
    "            device=device,\n",
    "            mode=hvp_mode,\n",
    "        )\n",
    "        l2_vals[start:end] = q_batch\n",
    "\n",
    "        if (b + 1) % 50 == 0 or b == n_batches - 1:\n",
    "            print(\n",
    "                f\"  processed batch {b+1}/{n_batches} ({end} / {n_edges} edges)\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    # 3.5) Clip extreme metric values (robust metric quantiles)\n",
    "    q_low = np.quantile(l2_vals, 0.05)\n",
    "    q_hi = np.quantile(l2_vals, 0.98)\n",
    "    l2_vals = np.clip(l2_vals, q_low, q_hi)\n",
    "\n",
    "    # 4) Choose kernel bandwidth ε\n",
    "    if eps_mode == \"median\":\n",
    "        eps = np.median(l2_vals)\n",
    "    elif eps_mode == \"fixed\":\n",
    "        eps = float(eps_value)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown eps_mode: {eps_mode}\")\n",
    "    print(f\"[EGGFM DiffMap] using eps = {eps:.4g}\", flush=True)\n",
    "\n",
    "    # 5) Build kernel W_ij = exp(-ℓ_ij^2 / eps)\n",
    "    W_vals = np.exp(-l2_vals / eps)\n",
    "    W = sparse.csr_matrix((W_vals, (rows, cols)), shape=(n_cells, n_cells))\n",
    "    # Symmetrize for robustness\n",
    "    W = 0.5 * (W + W.T)\n",
    "\n",
    "    # 6) Normalize to Markov matrix P (row-stochastic)\n",
    "    d = np.array(W.sum(axis=1)).ravel()\n",
    "    d_safe = np.maximum(d, 1e-12)\n",
    "    D_inv = sparse.diags(1.0 / d_safe)\n",
    "    P = D_inv @ W\n",
    "\n",
    "    # 7) Eigendecompose P^T for diffusion map\n",
    "    k_eigs = n_comps + 1  # include trivial eigenpair\n",
    "    print(\"[EGGFM DiffMap] computing eigenvectors...\", flush=True)\n",
    "    eigvals, eigvecs = eigs(P.T, k=k_eigs, which=\"LR\")  # largest real parts\n",
    "\n",
    "    eigvals = eigvals.real\n",
    "    eigvecs = eigvecs.real\n",
    "\n",
    "    # sort by eigenvalue magnitude descending\n",
    "    order = np.argsort(-eigvals)\n",
    "    eigvals = eigvals[order]\n",
    "    eigvecs = eigvecs[:, order]\n",
    "\n",
    "    # drop trivial eigenvector (λ≈1)\n",
    "    lambdas = eigvals[1 : n_comps + 1]\n",
    "    phis = eigvecs[:, 1 : n_comps + 1]  # (n_cells, n_comps)\n",
    "\n",
    "    # Diffusion map coordinates Ψ_t(x_i) = (λ_1^t φ_1(i), ..., λ_m^t φ_m(i))\n",
    "    diff_coords = phis * (lambdas**t)\n",
    "\n",
    "    print(\"[EGGFM DiffMap] finished. Embedding shape:\", diff_coords.shape, flush=True)\n",
    "    return diff_coords.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ef47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_model.py\n",
    "\n",
    "from typing import Sequence, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class EnergyMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    E(x) = <E_theta(x), x> where E_theta is an MLP with nonlinearities.\n",
    "\n",
    "    x is HVG, log-normalized expression (optionally mean-centered).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_genes: int,\n",
    "        hidden_dims: Sequence[int] = (512, 512, 512, 512),\n",
    "        activation: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if activation is None:\n",
    "            activation = nn.Softplus()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = n_genes\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(activation)\n",
    "            in_dim = h\n",
    "        # final layer outputs a vector in R^D\n",
    "        layers.append(nn.Linear(in_dim, n_genes))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, D)\n",
    "        returns: energy (B,)\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        v = self.net(x)  # (B, D) = E_theta(x)\n",
    "        energy = (v * x).sum(dim=-1)  # <E_theta(x), x>\n",
    "        return energy\n",
    "\n",
    "    def score(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        score(x) ≈ ∇_x log p(x) = -∇_x E(x)\n",
    "        \"\"\"\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        energy = self.forward(x)  # (B,)\n",
    "        energy_sum = energy.sum()\n",
    "        (grad,) = torch.autograd.grad(\n",
    "            energy_sum,\n",
    "            x,\n",
    "            create_graph=False,\n",
    "            retain_graph=False,\n",
    "            only_inputs=True,\n",
    "        )\n",
    "        score = -grad\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73d7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_energy.py\n",
    "\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_energy_model(\n",
    "    ad_prep,  # output of prep(ad, params)\n",
    "    model_cfg: Dict[str, Any],\n",
    "    train_cfg: Dict[str, Any],\n",
    ") -> EnergyMLP:\n",
    "    \"\"\"\n",
    "    Train an energy-based model on preprocessed AnnData using denoising score matching.\n",
    "\n",
    "    model_cfg (from params['eggfm_model']), e.g.:\n",
    "        hidden_dims: [512, 512, 512, 512]\n",
    "\n",
    "    train_cfg (from params['eggfm_train']), e.g.:\n",
    "        batch_size: 2048\n",
    "        num_epochs: 50\n",
    "        lr: 1e-4\n",
    "        sigma: 0.1\n",
    "        device: \"cuda\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Device\n",
    "    device = train_cfg.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset\n",
    "    dataset = AnnDataExpressionDataset(ad_prep)\n",
    "    n_genes = dataset.X.shape[1]\n",
    "\n",
    "    # Model\n",
    "    hidden_dims = model_cfg.get(\"hidden_dims\", (512, 512, 512, 512))\n",
    "    model = EnergyMLP(\n",
    "        n_genes=n_genes,\n",
    "        hidden_dims=hidden_dims,\n",
    "    ).to(device)\n",
    "\n",
    "    # Training hyperparameters (YAML overrides defaults)\n",
    "    batch_size = int(train_cfg.get(\"batch_size\", 2048))\n",
    "    num_epochs = int(train_cfg.get(\"num_epochs\", 50))\n",
    "    lr = float(train_cfg.get(\"lr\", 1e-4))\n",
    "    sigma = float(train_cfg.get(\"sigma\", 0.1))\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in loader:\n",
    "            x = batch.to(device)  # (B, D)\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(x)\n",
    "            y = x + sigma * eps\n",
    "            y.requires_grad_(True)\n",
    "\n",
    "            # Predicted score at y: s_theta(y) = -∇_y E(y)\n",
    "            energy = model(y)  # (B,)\n",
    "            energy_sum = energy.sum()\n",
    "            (grad_y,) = torch.autograd.grad(\n",
    "                energy_sum,\n",
    "                y,\n",
    "                create_graph=False,\n",
    "                retain_graph=False,\n",
    "                only_inputs=True,\n",
    "            )\n",
    "            s_theta = -grad_y  # (B, D)\n",
    "\n",
    "            # DSM target: -(y - x) / sigma^2\n",
    "            target = -(y - x) / (sigma**2)\n",
    "\n",
    "            # MSE over batch and dimensions\n",
    "            loss = ((s_theta - target) ** 2).sum(dim=1).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        print(\n",
    "            f\"[Energy DSM] Epoch {epoch+1}/{num_epochs}  loss={epoch_loss:.4f}\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b2547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eggfm_dimred(\n",
    "    qc_ad: sc.AnnData,\n",
    "    params: Dict[str, Any],\n",
    ") -> Tuple[sc.AnnData, object]:\n",
    "    \"\"\"\n",
    "    Run EGGFM-based dimension reduction on a preprocessed AnnData.\n",
    "\n",
    "    Assumes qc_ad is already:\n",
    "      - gene-filtered\n",
    "      - HVG-selected\n",
    "      - normalized + log1p\n",
    "\n",
    "    Config sections (from params.yml):\n",
    "      eggfm_model:\n",
    "        hidden_dims: [512, 512, 512, 512]\n",
    "\n",
    "      eggfm_train:\n",
    "        batch_size: 2048\n",
    "        num_epochs: 50\n",
    "        lr: 1e-4\n",
    "        sigma: 0.1\n",
    "        device: \"cuda\"\n",
    "\n",
    "      eggfm_diffmap:\n",
    "        n_neighbors: 10\n",
    "        n_comps: 30\n",
    "        device: \"cuda\"\n",
    "        hvp_mode: \"vHv\"\n",
    "        hvp_batch_size: 2048\n",
    "        eps_mode: \"median\"\n",
    "        eps_value: 1.0\n",
    "        t: 1.0\n",
    "    \"\"\"\n",
    "\n",
    "    # Let the train/diffmap functions handle their own defaults.\n",
    "    model_cfg = params.get(\"eggfm_model\", {})\n",
    "    train_cfg = params.get(\"eggfm_train\", {})\n",
    "    diff_cfg = params.get(\"eggfm_diffmap\", {})\n",
    "\n",
    "    # 1) Train energy model\n",
    "    energy_model = train_energy_model(qc_ad, model_cfg, train_cfg)\n",
    "\n",
    "    # 2) Build EGGFM DiffMap embedding (no subsampling)\n",
    "    X_eggfm = build_eggfm_diffmap(qc_ad, energy_model, diff_cfg)\n",
    "    qc_ad.obsm[\"X_eggfm\"] = X_eggfm\n",
    "    qc_ad.obsm[\"X_diff_eggfm\"] = X_eggfm  # make sure clustering sees this\n",
    "    qc_ad.uns[\"eggfm_meta\"] = {\n",
    "        \"hidden_dims\": model_cfg.get(\"hidden_dims\"),\n",
    "        \"batch_size\": train_cfg.get(\"batch_size\"),\n",
    "        \"lr\": train_cfg.get(\"lr\"),\n",
    "        \"sigma\": train_cfg.get(\"sigma\"),\n",
    "        \"n_neighbors\": diff_cfg.get(\"n_neighbors\"),\n",
    "        \"hvp_mode\": diff_cfg.get(\"hvp_mode\"),\n",
    "    }\n",
    "    return qc_ad, energy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "643cef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def prep_for_manifolds(\n",
    "    ad: sc.AnnData,\n",
    "    min_genes: int = 200,\n",
    "    hvg_n_top_genes: int = 2000,\n",
    "    min_cells_frac: float = 0.001,\n",
    ") -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    Preprocessing that mirrors your real `prep` function:\n",
    "\n",
    "      - QC metrics\n",
    "      - filter genes by min_cells_frac * n_cells\n",
    "      - filter cells by min_genes\n",
    "      - drop zero-total cells\n",
    "      - HVG selection (Seurat v3, subset=False)\n",
    "      - subset to HVGs\n",
    "      - normalize_total + log1p\n",
    "\n",
    "    No PCA here; we keep the data nonlinear for downstream DR.\n",
    "    \"\"\"\n",
    "    n_cells = ad.n_obs\n",
    "    print(\"[prep_for_manifolds] running Scanpy QC metrics\", flush=True)\n",
    "    sc.pp.calculate_qc_metrics(ad, inplace=True)\n",
    "\n",
    "    # Remove genes that are not statistically relevant (< min_cells_frac of cells)\n",
    "    min_cells = max(3, int(min_cells_frac * n_cells))\n",
    "    sc.pp.filter_genes(ad, min_cells=min_cells)\n",
    "\n",
    "    # Remove empty droplets / low-complexity cells\n",
    "    sc.pp.filter_cells(ad, min_genes=min_genes)\n",
    "\n",
    "    # Drop zero-count cells\n",
    "    totals = np.ravel(ad.X.sum(axis=1))\n",
    "    ad = ad[totals > 0, :].copy()\n",
    "\n",
    "    print(\"n_obs, n_vars (pre-HVG):\", ad.n_obs, ad.n_vars, flush=True)\n",
    "\n",
    "    # Explicit mean check, like in your script\n",
    "    X = ad.X\n",
    "    if sparse.issparse(X):\n",
    "        means = np.asarray(X.mean(axis=0)).ravel()\n",
    "    else:\n",
    "        means = np.nanmean(X, axis=0)\n",
    "\n",
    "    print(\"Means finite?\", np.all(np.isfinite(means)), flush=True)\n",
    "    print(\"Means min/max:\", np.nanmin(means), np.nanmax(means), flush=True)\n",
    "    print(\"# non-finite means:\", np.sum(~np.isfinite(means)), flush=True)\n",
    "\n",
    "    # HVG selection on raw X (no raw layer here)\n",
    "    sc.pp.highly_variable_genes(\n",
    "        ad,\n",
    "        n_top_genes=int(hvg_n_top_genes),\n",
    "        flavor=\"seurat_v3\",\n",
    "        subset=False,\n",
    "    )\n",
    "\n",
    "    ad = ad[:, ad.var[\"highly_variable\"]].copy()\n",
    "    print(\"n_obs, n_vars (post-HVG):\", ad.n_obs, ad.n_vars, flush=True)\n",
    "\n",
    "    # Now normalize/log on X\n",
    "    sc.pp.normalize_total(ad, target_sum=1e4)\n",
    "    sc.pp.log1p(ad)\n",
    "\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47cb4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "# Optional plotting / clustering helpers (for plot_result)\n",
    "# Comment these out if you don't need them.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _standardize(X: np.ndarray, axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center and scale X along the given axis (like R's scale()).\n",
    "    axis=0 => column-wise standardization.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    mean = X.mean(axis=axis, keepdims=True)\n",
    "    std = X.std(axis=axis, ddof=1, keepdims=True)\n",
    "    std[std == 0] = 1.0\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "###########################################################\n",
    "##### Utilities used in both DCOL-PCA & DCOL-CCA ##########\n",
    "###########################################################\n",
    "\n",
    "\n",
    "def scol_matrix_order(a: np.ndarray, x: np.ndarray) -> np.ndarray | float:\n",
    "    \"\"\"\n",
    "    Python version of scol.matrix.order(a, x).\n",
    "\n",
    "    x: 1D array used to order samples.\n",
    "    a: either a vector of length n_samples or a matrix with shape (n_rows, n_samples).\n",
    "       Returns:\n",
    "         - scalar if a is a vector / single-row\n",
    "         - 1D array of length n_rows if a is 2D (row-wise DCOL distances).\n",
    "    \"\"\"\n",
    "    a = np.asarray(a)\n",
    "    x = np.asarray(x)\n",
    "    order = np.argsort(x)\n",
    "\n",
    "    # a is effectively a vector (R's \"is.null(nrow(a)) | nrow(a) == 1\")\n",
    "    if a.ndim == 1 or a.shape[0] == 1:\n",
    "        a_vec = a.ravel()[order]\n",
    "        d = np.diff(a_vec)\n",
    "        dd = np.sum(d**2)\n",
    "        return float(dd)\n",
    "\n",
    "    # otherwise: matrix case, rows = features, cols = samples\n",
    "    a_sorted = a[:, order]\n",
    "    d = np.diff(a_sorted, axis=1)\n",
    "    dd = np.sum(d**2, axis=1)  # rowSums\n",
    "    return dd\n",
    "\n",
    "\n",
    "def find_dcol(a: np.ndarray, b: np.ndarray, n_nodes: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Python version of findDCOL(a, b, nNodes).\n",
    "\n",
    "    a, b: 2D arrays with shape (n_rows, n_samples).\n",
    "          Rows are features, columns are samples.\n",
    "    n_nodes: kept for API parity; current implementation is sequential.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dcol : np.ndarray, shape (nrow(a), nrow(b))\n",
    "        Symmetric DCOL distance matrix when a and b refer to the same set.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "\n",
    "    # vector vs vector case\n",
    "    if a.ndim == 1 or a.shape[0] == 1:\n",
    "        # a, b are treated as vectors\n",
    "        return np.array(scol_matrix_order(a, b), ndmin=1)\n",
    "\n",
    "    n_a = a.shape[0]\n",
    "    n_b = b.shape[0]\n",
    "\n",
    "    # NOTE: for simplicity, this is sequential. You can parallelize these loops\n",
    "    # with multiprocessing / joblib if needed.\n",
    "    dcolab = np.zeros((n_a, n_b), dtype=float)\n",
    "    dcolba = np.zeros((n_a, n_b), dtype=float)\n",
    "\n",
    "    # dcolab[i_column] = scol_matrix_order(a, b[i, ])\n",
    "    for i in range(n_b):\n",
    "        dcolab[:, i] = scol_matrix_order(a, b[i, :])\n",
    "\n",
    "    # dcolba[j_row] = scol_matrix_order(b, a[j, ])\n",
    "    for j in range(n_a):\n",
    "        dcolba[j, :] = scol_matrix_order(b, a[j, :])\n",
    "\n",
    "    # retain the smaller entry to enforce symmetry\n",
    "    dcol = np.minimum(dcolab, dcolba)\n",
    "    return dcol\n",
    "\n",
    "\n",
    "def get_cov(\n",
    "    dcol_matrix: np.ndarray, X: np.ndarray, Y: np.ndarray\n",
    ") -> np.ndarray | float:\n",
    "    \"\"\"\n",
    "    Python version of getCov(DCOLMatrix, X, Y).\n",
    "\n",
    "    X, Y: data matrices with shape (n_samples, n_features), rows = samples.\n",
    "    dcol_matrix:\n",
    "      - scalar / length-1 => vector case (single pair).\n",
    "      - 2D matrix (p x p) => DCOL distances between features (columns of Y).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - scalar in the vector case\n",
    "    - CovMatrix (DCOL-correlation matrix), same shape as dcol_matrix in matrix case.\n",
    "    \"\"\"\n",
    "    dcol = np.asarray(dcol_matrix, float)\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "\n",
    "    # Vector / scalar case\n",
    "    if dcol.ndim == 0 or (dcol.ndim == 1 and dcol.shape[0] == 1):\n",
    "        X = X.ravel()\n",
    "        Y = Y.ravel()\n",
    "        n = X.shape[0]\n",
    "        var_Y = np.var(Y, ddof=1)\n",
    "        if var_Y <= 0:\n",
    "            # Degenerate case: no variance in Y, return 0 correlation\n",
    "            return 0.0\n",
    "        value = np.sqrt(max(0.0, 1.0 - dcol.item() / (2.0 * (n - 2.0) * var_Y)))\n",
    "        return float(value)\n",
    "\n",
    "    # Matrix case\n",
    "    n = X.shape[0]\n",
    "    var_list = np.var(Y, axis=0, ddof=1)  # sample variance over samples\n",
    "\n",
    "    eps = 1e-12\n",
    "    zero_var = var_list <= eps\n",
    "    if np.any(zero_var):\n",
    "        print(\n",
    "            f\"[get_cov] {zero_var.sum()} zero-variance features; stabilizing\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    scale = np.zeros_like(var_list)\n",
    "    ok = ~zero_var\n",
    "    scale[ok] = 1.0 / (2.0 * (n - 2.0) * var_list[ok])\n",
    "\n",
    "    # eigenMapMatMult(DCOLMatrix, diag(scale)) == column-wise scaling by 'scale'\n",
    "    cov_matrix = 1.0 - dcol * scale  # broadcast scale across rows\n",
    "    cov_matrix[cov_matrix < 0] = 0.0  # clamp negs to 0 for num stability\n",
    "\n",
    "    # For zero-var features, zero out row/cl and set diag to 1\n",
    "    if np.any(zero_var):\n",
    "        cov_matrix[:, zero_var] = 0.0\n",
    "        cov_matrix[zero_var, :] = 0.0\n",
    "        idx = np.where(zero_var)[0]\n",
    "        cov_matrix[idx, idx] = 1.0\n",
    "\n",
    "    cov_matrix = np.sqrt(cov_matrix)\n",
    "    return cov_matrix\n",
    "\n",
    "\n",
    "###########################################################\n",
    "##### DCOL-PCA (feature-based and cell-based versions) ####\n",
    "###########################################################\n",
    "\n",
    "\n",
    "def dcol_pca0(\n",
    "    X: np.ndarray,\n",
    "    image: int = 0,\n",
    "    k: int = 4,\n",
    "    labels=None,\n",
    "    Scale: bool = True,\n",
    "    nNodes: int = 1,\n",
    "    nPC_max: int = 100,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python version of Dcol_PCA0(X, ...).\n",
    "    PCA with n = cells and k = principal k features\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Columns are features (genes), rows are samples (cells).\n",
    "    image : int\n",
    "        If 1, you can add plotting code here (not implemented by default).\n",
    "    k : int\n",
    "        Number of dimensions to keep for 'data.r' (visualization).\n",
    "    labels : array-like, optional\n",
    "        Group labels for plotting (unused unless you add plotting).\n",
    "    Scale : bool\n",
    "        Whether to standardize features before computing DCOL.\n",
    "    nNodes : int\n",
    "        Kept for API compatibility; current implementation is sequential.\n",
    "    nPC_max : int\n",
    "        Maximum number of principal components to compute.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "      - 'cov_D'      : DCOL-based correlation matrix (p x p)\n",
    "      - 'vecs'   : eigenvectors of cov_D (p x nPC)\n",
    "      - 'vals'    : eigenvalues (nPC,)\n",
    "      - 'data_r'     : embedding (n_samples x min(k, nPC))\n",
    "      - 'X_proj'     : full projection (n_samples x nPC)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    X_o = X.copy()  # used for final projection\n",
    "\n",
    "    if Scale:\n",
    "        X = _standardize(X, axis=0)  # column-wise (features)\n",
    "\n",
    "    # DCOL matrix over features: findDCOL(t(X), t(X))\n",
    "    DcolMatrix = find_dcol(X.T, X.T, n_nodes=nNodes)\n",
    "    cov_D = get_cov(DcolMatrix, X, X)  # DCOL-correlation matrix\n",
    "\n",
    "    # Suppose cov_D is the matrix passed to eigsh\n",
    "    print(\"[dcol_pca] cov_D finite?\", np.isfinite(cov_D).all(), flush=True)\n",
    "    print(\"[dcol_pca] cov_D min/max:\", np.nanmin(cov_D), np.nanmax(cov_D), flush=True)\n",
    "\n",
    "    # Eigen-decomposition (like RSpectra::eigs_sym on symmetric cov_D)\n",
    "    p = cov_D.shape[0]\n",
    "    nPC = min(nPC_max, p)\n",
    "\n",
    "    # Enforce symmetry and remove NaN/Inf just in case\n",
    "    cov_D = 0.5 * (cov_D + cov_D.T)\n",
    "    cov_D = np.nan_to_num(cov_D, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Add tiny diagonal jitter for numerical stablility\n",
    "    cov_D.flat[:: p + 1] += 1e-8\n",
    "\n",
    "    if p <= nPC + 10:\n",
    "        # small matrix: use dense eigh\n",
    "        vals, vecs = np.linalg.eigh(cov_D)\n",
    "        idx = np.argsort(vals)[::-1][:nPC]\n",
    "        vals = vals[idx]\n",
    "        vecs = vecs[:, idx]\n",
    "    else:\n",
    "        # large matrix: sparse eigensolver\n",
    "        vals, vecs = eigsh(cov_D, k=nPC, which=\"LM\")\n",
    "        idx = np.argsort(vals)[::-1]\n",
    "        vals = vals[idx]\n",
    "        vecs = vecs[:, idx]\n",
    "\n",
    "    # Project original (unscaled) X onto eigenvectors\n",
    "    X_proj = X_o @ vecs  # (n_samples x nPC)\n",
    "    data_r = X_proj[:, : min(k, X_proj.shape[1])]\n",
    "\n",
    "    # You can add plotting here if image == 1 (using matplotlib).\n",
    "\n",
    "    return {\n",
    "        \"cov_D\": cov_D,\n",
    "        \"vecs\": vecs,\n",
    "        \"vals\": vals,\n",
    "        \"data_r\": data_r,\n",
    "        \"X_proj\": X_proj,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66eb3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_manifolds(\n",
    "    adata: sc.AnnData,\n",
    "    params: Dict[str, Any],\n",
    "    n_pcs: int = 50,\n",
    "    seed: int = 0,\n",
    "    label_key: str | None = None,\n",
    "    n_neighbors: int = 30,\n",
    "    umap_min_dist: float = 0.3,\n",
    ") -> Tuple[sc.AnnData, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute a small panel of embeddings on `adata` and optionally display\n",
    "    a UMAP for each manifold inline (Jupyter style).\n",
    "\n",
    "    Embeddings stored / returned:\n",
    "      - dcol_pca       -> adata.obsm[\"X_dcolpca\"]\n",
    "      - pca            -> adata.obsm[\"X_pca\"]\n",
    "      - diffmap_pca    -> adata.obsm[\"X_diff_pca\"]\n",
    "      - diffmap_dcol   -> adata.obsm[\"X_diff_dcol\"]\n",
    "      - diffmap_eggfm  -> adata.obsm[\"X_diff_eggfm\"]\n",
    "      - phate          -> adata.obsm[\"X_phate\"]      (if phate installed)\n",
    "      - scvi           -> adata.obsm[\"X_scvi\"]       (if scvi-tools installed)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : AnnData\n",
    "        Input AnnData (assumed already preprocessed: HVGs, scale, etc.).\n",
    "    params : dict\n",
    "        Full params dict (e.g., from yaml.safe_load) containing eggfm_* configs.\n",
    "    n_pcs : int\n",
    "        Number of components for PCA / Diffmap / scVI latent.\n",
    "    seed : int\n",
    "        Random seed for PCA / neighbors / UMAP.\n",
    "    label_key : str or None\n",
    "        Column in `adata.obs` to color UMAPs by. If None, UMAPs are not plotted.\n",
    "    n_neighbors : int\n",
    "        k for k-NN graph used in UMAP / Diffmap.\n",
    "    umap_min_dist : float\n",
    "        `min_dist` parameter for UMAP.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata : AnnData\n",
    "        Same object with embeddings stored in .obsm.\n",
    "    embeddings : dict[str, np.ndarray]\n",
    "        Mapping from embedding name to the embedding array.\n",
    "    \"\"\"\n",
    "    embeddings: Dict[str, np.ndarray] = {}\n",
    "    spec = params.get(\"spec\", {})\n",
    "    dcol_max_cells = int(spec.get(\"dcol_max_cells\", 3000))\n",
    "\n",
    "    # # ----------------------------------------------------\n",
    "    # # 0) DCOL-PCA (fit on subset, project all cells)\n",
    "    # # ----------------------------------------------------\n",
    "    if adata.n_obs > dcol_max_cells:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = np.sort(rng.choice(adata.n_obs, size=dcol_max_cells, replace=False))\n",
    "        ad_dcol = adata[idx, :].copy()\n",
    "        print(\n",
    "            f\"[dcol_pca] subsampled {dcol_max_cells}/{adata.n_obs} cells for DCOL-PCA\",\n",
    "            flush=True,\n",
    "        )\n",
    "    else:\n",
    "        ad_dcol = adata.copy()\n",
    "        print(f\"[dcol_pca] using all {adata.n_obs} cells for DCOL-PCA\", flush=True)\n",
    "\n",
    "    X_sub = ad_dcol.X\n",
    "    if sparse.issparse(X_sub):\n",
    "        X_sub = X_sub.toarray()\n",
    "\n",
    "    K_sub = dcol_pca0(X_sub, nPC_max=n_pcs, Scale=True)\n",
    "    vecs_dcol = K_sub[\"vecs\"]  # (n_genes x n_pcs)\n",
    "\n",
    "    X_full = adata.X\n",
    "    if sparse.issparse(X_full):\n",
    "        X_full = X_full.toarray()\n",
    "    X_proj_full_dcol = X_full @ vecs_dcol\n",
    "    adata.obsm[\"X_dcolpca\"] = X_proj_full_dcol\n",
    "    embeddings[\"dcol_pca\"] = X_proj_full_dcol\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1) PCA (baseline)\n",
    "    # ----------------------------------------------------\n",
    "    if \"X_pca\" not in adata.obsm_keys():\n",
    "        sc.tl.pca(adata, n_comps=n_pcs, svd_solver=\"arpack\", random_state=seed)\n",
    "    embeddings[\"pca\"] = adata.obsm[\"X_pca\"].copy()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2) Diffusion maps (PCA prior & DCOL prior)\n",
    "    # ----------------------------------------------------\n",
    "    # (a) Diffmap with PCA prior\n",
    "    sc.pp.neighbors(adata, n_neighbors=n_neighbors, use_rep=\"X_pca\", random_state=seed)\n",
    "    sc.tl.diffmap(adata, n_comps=n_pcs)\n",
    "    X_diff_pca = adata.obsm[\"X_diffmap\"][:, :n_pcs].copy()\n",
    "    adata.obsm[\"X_diff_pca\"] = X_diff_pca\n",
    "    embeddings[\"diffmap_pca\"] = X_diff_pca\n",
    "\n",
    "    # (b) Diffmap with DCOL prior\n",
    "    sc.pp.neighbors(\n",
    "        adata, n_neighbors=n_neighbors, use_rep=\"X_dcolpca\", random_state=seed\n",
    "    )\n",
    "    sc.tl.diffmap(adata, n_comps=n_pcs)\n",
    "    X_diff_dcol = adata.obsm[\"X_diffmap\"][:, :n_pcs].copy()\n",
    "    adata.obsm[\"X_diff_dcol\"] = X_diff_dcol\n",
    "    embeddings[\"diffmap_dcol\"] = X_diff_dcol\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3) EGGFM Diffmap\n",
    "    # ----------------------------------------------------\n",
    "    # This will write X_eggfm and X_diff_eggfm into .obsm\n",
    "    adata, _ = run_eggfm_dimred(adata, params)\n",
    "    if \"X_diff_eggfm\" in adata.obsm:\n",
    "        X_diff_eggfm = adata.obsm[\"X_diff_eggfm\"]\n",
    "        if X_diff_eggfm.shape[1] > n_pcs:\n",
    "            X_diff_eggfm = X_diff_eggfm[:, :n_pcs]\n",
    "            adata.obsm[\"X_diff_eggfm\"] = X_diff_eggfm\n",
    "        embeddings[\"diffmap_eggfm\"] = X_diff_eggfm\n",
    "    else:\n",
    "        print(\"[eggfm] X_diff_eggfm not found in adata.obsm; skipping.\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4) PHATE (optional, on X, no PCA first)\n",
    "    # ----------------------------------------------------\n",
    "    try:\n",
    "        import phate\n",
    "\n",
    "        X_ph = adata.X\n",
    "        if sparse.issparse(X_ph):\n",
    "            X_ph = X_ph.toarray()\n",
    "\n",
    "        phate_op = phate.PHATE(\n",
    "            n_components=n_pcs,\n",
    "            n_jobs=-1,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        X_phate = phate_op.fit_transform(X_ph)\n",
    "        adata.obsm[\"X_phate\"] = X_phate\n",
    "        embeddings[\"phate\"] = X_phate\n",
    "    except ImportError:\n",
    "        print(\"[warn] phate not installed; skipping PHATE embedding\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 5) scVI latent space (optional)\n",
    "    # ----------------------------------------------------\n",
    "    try:\n",
    "        import scvi\n",
    "\n",
    "        scvi.model.SCVI.setup_anndata(adata)\n",
    "        scvi_model = scvi.model.SCVI(adata, n_latent=min(10, n_pcs))\n",
    "        scvi_model.train(\n",
    "            max_epochs=200,\n",
    "            check_val_every_n_epoch=10,\n",
    "            plan_kwargs={\"lr\": 1e-3},\n",
    "        )\n",
    "        X_scvi = scvi_model.get_latent_representation()\n",
    "        adata.obsm[\"X_scvi\"] = X_scvi\n",
    "        embeddings[\"scvi\"] = X_scvi\n",
    "    except ImportError:\n",
    "        print(\"[warn] scvi-tools not installed; skipping scVI latent\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 6) UMAPs for each embedding (inline, Jupyter style)\n",
    "    # ----------------------------------------------------\n",
    "    if label_key is not None:\n",
    "        methods = {\n",
    "            \"X_pca\": \"PCA\",\n",
    "            \"X_dcolpca\": \"DCOL-PCA\",\n",
    "            \"X_diff_pca\": \"Diffmap (PCA)\",\n",
    "            \"X_diff_dcol\": \"Diffmap (DCOL)\",\n",
    "            \"X_diff_eggfm\": \"Diffmap (EGGFM)\",\n",
    "            \"X_phate\": \"PHATE\",\n",
    "            \"X_scvi\": \"scVI\",\n",
    "        }\n",
    "\n",
    "        for obsm_key, title in methods.items():\n",
    "            if obsm_key not in adata.obsm:\n",
    "                print(f\"[UMAP] skipping {obsm_key}, not in adata.obsm\")\n",
    "                continue\n",
    "\n",
    "            print(f\"[UMAP] {title} → UMAP({label_key})\")\n",
    "            ad_tmp = adata.copy()\n",
    "            ad_tmp.obsm[\"X_tmp\"] = adata.obsm[obsm_key]\n",
    "\n",
    "            sc.pp.neighbors(\n",
    "                ad_tmp,\n",
    "                n_neighbors=n_neighbors,\n",
    "                use_rep=\"X_tmp\",\n",
    "                random_state=seed,\n",
    "            )\n",
    "            sc.tl.umap(ad_tmp, min_dist=umap_min_dist, random_state=seed)\n",
    "\n",
    "            sc.pl.umap(\n",
    "                ad_tmp,\n",
    "                color=label_key,\n",
    "                title=f\"{title} → UMAP ({label_key})\",\n",
    "                frameon=False,\n",
    "                show=True,\n",
    "            )\n",
    "\n",
    "    return adata, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8894234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prep_for_manifolds] running Scanpy QC metrics\n",
      "n_obs, n_vars (pre-HVG): 2730 3451\n",
      "Means finite? True\n",
      "Means min/max: 0.0043956046 26.31099\n",
      "# non-finite means: 0\n",
      "n_obs, n_vars (post-HVG): 2730 2000\n",
      "[dcol_pca] using all 2730 cells for DCOL-PCA\n",
      "[dcol_pca] cov_D finite? True\n",
      "[dcol_pca] cov_D min/max: 0.0 0.999979414318045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2081553616.py:83: FutureWarning: Use obsm (e.g. `k in adata.obsm` or `adata.obsm.keys() | {'u'}`) instead of AnnData.obsm_keys, AnnData.obsm_keys is deprecated and will be removed in the future.\n",
      "  if \"X_pca\" not in adata.obsm_keys():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Energy DSM] Epoch 1/50  loss=0.0000\n",
      "[Energy DSM] Epoch 2/50  loss=0.0000\n",
      "[Energy DSM] Epoch 3/50  loss=0.0000\n",
      "[Energy DSM] Epoch 4/50  loss=0.0000\n",
      "[Energy DSM] Epoch 5/50  loss=0.0000\n",
      "[Energy DSM] Epoch 6/50  loss=0.0000\n",
      "[Energy DSM] Epoch 7/50  loss=0.0000\n",
      "[Energy DSM] Epoch 8/50  loss=0.0000\n",
      "[Energy DSM] Epoch 9/50  loss=0.0000\n",
      "[Energy DSM] Epoch 10/50  loss=0.0000\n",
      "[Energy DSM] Epoch 11/50  loss=0.0000\n",
      "[Energy DSM] Epoch 12/50  loss=0.0000\n",
      "[Energy DSM] Epoch 13/50  loss=0.0000\n",
      "[Energy DSM] Epoch 14/50  loss=0.0000\n",
      "[Energy DSM] Epoch 15/50  loss=0.0000\n",
      "[Energy DSM] Epoch 16/50  loss=0.0000\n",
      "[Energy DSM] Epoch 17/50  loss=0.0000\n",
      "[Energy DSM] Epoch 18/50  loss=0.0000\n",
      "[Energy DSM] Epoch 19/50  loss=0.0000\n",
      "[Energy DSM] Epoch 20/50  loss=0.0000\n",
      "[Energy DSM] Epoch 21/50  loss=0.0000\n",
      "[Energy DSM] Epoch 22/50  loss=0.0000\n",
      "[Energy DSM] Epoch 23/50  loss=0.0000\n",
      "[Energy DSM] Epoch 24/50  loss=0.0000\n",
      "[Energy DSM] Epoch 25/50  loss=0.0000\n",
      "[Energy DSM] Epoch 26/50  loss=0.0000\n",
      "[Energy DSM] Epoch 27/50  loss=0.0000\n",
      "[Energy DSM] Epoch 28/50  loss=0.0000\n",
      "[Energy DSM] Epoch 29/50  loss=0.0000\n",
      "[Energy DSM] Epoch 30/50  loss=0.0000\n",
      "[Energy DSM] Epoch 31/50  loss=0.0000\n",
      "[Energy DSM] Epoch 32/50  loss=0.0000\n",
      "[Energy DSM] Epoch 33/50  loss=0.0000\n",
      "[Energy DSM] Epoch 34/50  loss=0.0000\n",
      "[Energy DSM] Epoch 35/50  loss=0.0000\n",
      "[Energy DSM] Epoch 36/50  loss=0.0000\n",
      "[Energy DSM] Epoch 37/50  loss=0.0000\n",
      "[Energy DSM] Epoch 38/50  loss=0.0000\n",
      "[Energy DSM] Epoch 39/50  loss=0.0000\n",
      "[Energy DSM] Epoch 40/50  loss=0.0000\n",
      "[Energy DSM] Epoch 41/50  loss=0.0000\n",
      "[Energy DSM] Epoch 42/50  loss=0.0000\n",
      "[Energy DSM] Epoch 43/50  loss=0.0000\n",
      "[Energy DSM] Epoch 44/50  loss=0.0000\n",
      "[Energy DSM] Epoch 45/50  loss=0.0000\n",
      "[Energy DSM] Epoch 46/50  loss=0.0000\n",
      "[Energy DSM] Epoch 47/50  loss=0.0000\n",
      "[Energy DSM] Epoch 48/50  loss=0.0000\n",
      "[Energy DSM] Epoch 49/50  loss=0.0000\n",
      "[Energy DSM] Epoch 50/50  loss=0.0000\n",
      "[EGGFM DiffMap] X shape: (2730, 2000)\n",
      "[EGGFM DiffMap] building kNN graph (euclidean for neighbor selection)...\n",
      "[EGGFM DiffMap] total edges (directed): 27300\n",
      "[EGGFM DiffMap] computing Hessian-based edge lengths in batches...\n",
      "  processed batch 7/7 (27300 / 27300 edges)\n",
      "[EGGFM DiffMap] using eps = nan\n",
      "[EGGFM DiffMap] computing eigenvectors...\n"
     ]
    },
    {
     "ename": "ArpackError",
     "evalue": "ARPACK error -9999: Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArpackError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1789109365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mlabel_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"paul15_clusters\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m adata, embeddings = compute_manifolds(\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2081553616.py\u001b[0m in \u001b[0;36mcompute_manifolds\u001b[0;34m(adata, params, n_pcs, seed, label_key, n_neighbors, umap_min_dist)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# ----------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# This will write X_eggfm and X_diff_eggfm into .obsm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eggfm_dimred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"X_diff_eggfm\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mX_diff_eggfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_diff_eggfm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2400629970.py\u001b[0m in \u001b[0;36mrun_eggfm_dimred\u001b[0;34m(qc_ad, params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# 2) Build EGGFM DiffMap embedding (no subsampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mX_eggfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_eggfm_diffmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqc_ad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mqc_ad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_eggfm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_eggfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mqc_ad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_diff_eggfm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_eggfm\u001b[0m  \u001b[0;31m# make sure clustering sees this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3133843029.py\u001b[0m in \u001b[0;36mbuild_eggfm_diffmap\u001b[0;34m(ad_prep, energy_model, diff_cfg)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mk_eigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_comps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# include trivial eigenpair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[EGGFM DiffMap] computing eigenvectors...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0meigvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_eigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LR\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# largest real parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0meigvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigs\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, OPpart)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_ARPACK_LOCK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_eigenvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_no_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mArpackError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfodict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_infodict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_eigenvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArpackError\u001b[0m: ARPACK error -9999: Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated."
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "adata = sc.datasets.paul15()\n",
    "\n",
    "# 2) Preprocess in a way that mirrors your real Weinreb prep\n",
    "adata = prep_for_manifolds(\n",
    "    adata,\n",
    "    min_genes=200,  # match params[\"qc\"][\"min_genes\"] if you want\n",
    "    hvg_n_top_genes=2000,  # match params[\"hvg_n_top_genes\"]\n",
    ")\n",
    "\n",
    "# 3) Compute manifolds + UMAPs using DCOL, Diffmap(PCA/DCOL/EGGFM), PHATE, scVI\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"seed\": 7,\n",
    "    \"hvg_n_top_genes\": 2000,\n",
    "    \"spec\": {\n",
    "        \"n_pcs\": 20,\n",
    "        \"dcol_max_cells\": 3000,\n",
    "        \"ari_label_key\": \"Cell type annotation\",  # must match adata.obs column\n",
    "        \"ari_n_dims\": 10,\n",
    "    },\n",
    "    \"qc\": {\n",
    "        \"min_cells\": 500,\n",
    "        \"min_genes\": 200,\n",
    "        \"max_pct_mt\": 15,\n",
    "    },\n",
    "    \"eggfm_model\": {\n",
    "        \"hidden_dims\": [512, 512, 512, 512],\n",
    "    },\n",
    "    \"eggfm_train\": {\n",
    "        \"batch_size\": 4096,\n",
    "        \"num_epochs\": 50,\n",
    "        \"lr\": 1e-4,\n",
    "        \"sigma\": 0.1,\n",
    "        \"device\": \"cuda\",\n",
    "    },\n",
    "    \"eggfm_diffmap\": {\n",
    "        \"n_neighbors\": 10,\n",
    "        \"n_comps\": 30,\n",
    "        \"device\": \"cuda\",\n",
    "        \"hvp_mode\": \"vHv\",\n",
    "        \"hvp_batch_size\": 4096,\n",
    "        \"eps_mode\": \"median\",\n",
    "        \"eps_value\": 1.0,\n",
    "        \"t\": 1.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "label_key = \"paul15_clusters\"\n",
    "\n",
    "adata, embeddings = compute_manifolds(\n",
    "    adata,\n",
    "    params=params,\n",
    "    n_pcs=50,\n",
    "    seed=0,\n",
    "    label_key=label_key,\n",
    "    n_neighbors=30,\n",
    "    umap_min_dist=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333118e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_embeddings(\n",
    "    embeddings: dict[str, np.ndarray],\n",
    "    labels: np.ndarray,\n",
    "    n_clusters: int,\n",
    "    n_seeds: int = 10,\n",
    "    seed: int = 0,\n",
    "    ari_k: int = 10,\n",
    "    plot: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a dict of embeddings via KMeans + ARI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dict[str, np.ndarray]\n",
    "        Mapping from method name -> embedding array (n_cells x d).\n",
    "    labels : np.ndarray\n",
    "        Ground-truth labels (length n_cells).\n",
    "    n_clusters : int\n",
    "        Number of clusters for KMeans (typically == # unique labels).\n",
    "    n_seeds : int\n",
    "        Number of KMeans random seeds to use for each embedding.\n",
    "    seed : int\n",
    "        Base random seed; seeds used are seed, seed+1, ..., seed+n_seeds-1.\n",
    "    ari_k : int\n",
    "        Number of leading dimensions to use from each embedding.\n",
    "    plot : bool\n",
    "        If True, show a bar plot of mean ARI with error bars (± std).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Columns: [\"method\", \"mean_ari\", \"std_ari\"] sorted by mean_ari desc.\n",
    "    \"\"\"\n",
    "    rows: list[dict[str, float | str]] = []\n",
    "\n",
    "    for name, emb in embeddings.items():\n",
    "        emb = np.asarray(emb)\n",
    "        if emb.ndim != 2:\n",
    "            print(f\"[evaluate_embeddings] skipping {name}: emb.ndim={emb.ndim}\")\n",
    "            continue\n",
    "        if emb.shape[0] != labels.shape[0]:\n",
    "            print(\n",
    "                f\"[evaluate_embeddings] skipping {name}: \"\n",
    "                f\"n_cells mismatch (emb={emb.shape[0]}, labels={labels.shape[0]})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        k_eff = min(ari_k, emb.shape[1])\n",
    "        X_use = emb[:, :k_eff]\n",
    "\n",
    "        scores = []\n",
    "        for s in range(seed, seed + n_seeds):\n",
    "            km = KMeans(n_clusters=n_clusters, n_init=10, random_state=s)\n",
    "            km.fit(X_use)\n",
    "            ari = adjusted_rand_score(labels, km.labels_)\n",
    "            scores.append(ari)\n",
    "\n",
    "        scores = np.asarray(scores, float)\n",
    "        mean_ari = float(scores.mean())\n",
    "        std_ari = float(scores.std())\n",
    "\n",
    "        print(f\"[ARI] {name:15s}: mean={mean_ari:0.4f}, std={std_ari:0.4f}\")\n",
    "        rows.append({\"method\": name, \"mean_ari\": mean_ari, \"std_ari\": std_ari})\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"[evaluate_embeddings] No valid embeddings to evaluate.\")\n",
    "\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"mean_ari\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.bar(df[\"method\"], df[\"mean_ari\"], yerr=df[\"std_ari\"], capsize=4)\n",
    "        ax.set_ylabel(\"Adjusted Rand Index\")\n",
    "        ax.set_title(f\"Clustering ARI across embeddings (k={ari_k}, seeds={n_seeds})\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import yaml\n",
    "\n",
    "\n",
    "def run_paul15_manifold_panel(\n",
    "    out: str = \"out/paul15_dimred_summary.csv\",\n",
    "    params_path: str | None = \"configs/params.yml\",\n",
    "    min_genes: int = 200,\n",
    "    hvg_n_top_genes: int = 2000,\n",
    "    n_pcs: int = 50,\n",
    "    seed: int = 0,\n",
    "    n_seeds: int = 10,\n",
    ") -> Tuple[sc.AnnData, Dict[str, np.ndarray], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run the Paul15 manifold panel inside a Jupyter notebook, using a\n",
    "    preprocessing and manifold pipeline that mirrors your Weinreb dim_red.\n",
    "\n",
    "    Steps:\n",
    "      1) Load sc.datasets.paul15()\n",
    "      2) Preprocess with prep_for_manifolds (HVGs, normalize, log1p; no PCA)\n",
    "      3) Compute manifolds via compute_manifolds (DCOL-PCA, Diffmap(PCA/DCOL/EGGFM),\n",
    "         PHATE, scVI) + optional UMAPs\n",
    "      4) Evaluate ARI stability across embeddings with KMeans\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata : AnnData\n",
    "        Preprocessed Paul15 dataset with embeddings stored in .obsm.\n",
    "    embeddings : dict[str, np.ndarray]\n",
    "        Dict of embedding name -> embedding array.\n",
    "    df : pd.DataFrame\n",
    "        ARI summary for each embedding.\n",
    "    \"\"\"\n",
    "    out_path = Path(out)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- load params (for EGGFM, dcol_max_cells, etc.) ---\n",
    "    if params_path is not None:\n",
    "        params: Dict[str, Any] = yaml.safe_load(Path(params_path).read_text())\n",
    "    else:\n",
    "        params = {\"spec\": {\"dcol_max_cells\": 3000}}  # minimal fallback\n",
    "\n",
    "    # --- 1) load and 2) preprocess like your real pipeline ---\n",
    "    adata = sc.datasets.paul15()\n",
    "    adata = prep_for_manifolds(\n",
    "        adata,\n",
    "        min_genes=min_genes,\n",
    "        hvg_n_top_genes=hvg_n_top_genes,\n",
    "        min_cells_frac=0.001,\n",
    "    )\n",
    "\n",
    "    # Ground-truth labels from Scanpy's paul15 dataset\n",
    "    labels_cat = adata.obs[\"paul15_clusters\"].astype(\"category\")\n",
    "    labels = labels_cat.cat.codes.to_numpy()\n",
    "    n_clusters = labels_cat.cat.categories.size\n",
    "\n",
    "    # --- 3) manifolds + UMAPs ---\n",
    "    adata, embeddings = compute_manifolds(\n",
    "        adata,\n",
    "        params=params,\n",
    "        n_pcs=n_pcs,\n",
    "        seed=seed,\n",
    "        label_key=\"paul15_clusters\",  # for UMAP coloring\n",
    "        n_neighbors=30,\n",
    "        umap_min_dist=0.3,\n",
    "    )\n",
    "\n",
    "    # --- 4) ARI evaluation ---\n",
    "    df = evaluate_embeddings(\n",
    "        embeddings,\n",
    "        labels,\n",
    "        n_clusters=n_clusters,\n",
    "        n_seeds=n_seeds,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved ARI summary to {out_path}\")\n",
    "    print(df)\n",
    "\n",
    "    return adata, embeddings, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d71800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_paul15, paul15_embeddings, paul15_df = run_paul15_manifold_panel(\n",
    "#     out=\"out/paul15_dimred_summary.csv\",\n",
    "#     n_top_genes=2000,\n",
    "#     n_pcs=50,\n",
    "#     seed=0,\n",
    "#     n_seeds=10,\n",
    "# )\n",
    "\n",
    "# Example for Paul15\n",
    "labels_cat = adata.obs[\"paul15_clusters\"].astype(\"category\")\n",
    "labels = labels_cat.cat.codes.to_numpy()\n",
    "n_clusters = labels_cat.cat.categories.size\n",
    "\n",
    "df_ari = evaluate_embeddings(\n",
    "    embeddings,\n",
    "    labels=labels,\n",
    "    n_clusters=n_clusters,\n",
    "    n_seeds=10,\n",
    "    seed=0,\n",
    "    ari_k=10,\n",
    "    plot=True,\n",
    ")\n",
    "df_ari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
